{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huggingface on SageMaker Pipeline\n",
    "### Binary Classification with `Trainer` and `imdb` dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Introduction](#Introduction)  \n",
    "2. [Development Environment and Permissions](#Development-Environment-and-Permissions)\n",
    "    1. [Installation](#Installation)  \n",
    "    2. [Development environment](#Development-environment)  \n",
    "    3. [Permissions](#Permissions)\n",
    "3. [Processing](#Preprocessing)   \n",
    "    1. [Tokenization](#Tokenization)  \n",
    "    2. [Uploading data to sagemaker_session_bucket](#Uploading-data-to-sagemaker_session_bucket)  \n",
    "4. [Fine-tuning & starting Sagemaker Training Job](#Fine-tuning-\\&-starting-Sagemaker-Training-Job)  \n",
    "    1. [Creating an Estimator and start a training job](#Creating-an-Estimator-and-start-a-training-job)  \n",
    "    2. [Estimator Parameters](#Estimator-Parameters)   \n",
    "    3. [Download fine-tuned model from s3](#Download-fine-tuned-model-from-s3)\n",
    "    3. [Attach to old training job to an estimator ](#Attach-to-old-training-job-to-an-estimator)  \n",
    "5. [_Coming soon_:Push model to the Hugging Face hub](#Push-model-to-the-Hugging-Face-hub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Welcome to our end-to-end binary Text-Classification example. In this demo, we will use the Hugging Faces `transformers` and `datasets` library together with a custom Amazon sagemaker-sdk extension to fine-tune a pre-trained transformer on binary text classification. In particular, the pre-trained model will be fine-tuned using the `imdb` dataset. To get started, we need to set up the environment with a few prerequisite steps, for permissions, configurations, and so on. \n",
    "\n",
    "This is an extend for this [get start demo](https://github.com/huggingface/notebooks/blob/main/sagemaker/01_getting_started_pytorch/sagemaker-notebook.ipynb), we add SageMaker Processing, SageMaker Batch Transform and SageMaker Pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development Environment and Permissions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "_*Note:* we only install the required libraries from Hugging Face and AWS. You also need PyTorch or Tensorflow, if you haven´t it installed_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install \"sagemaker>=2.48.0\" \"transformers==4.12.3\" \"datasets[s3]==1.18.3\" --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Development environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker.huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_If you are going to use Sagemaker in a local environment. You need access to an IAM Role with the required permissions for Sagemaker. You can find [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) more about it._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "base_job_name = 'huggingfaces-sm-demo'\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "We are using the `datasets` library to download and preprocess the `imdb` dataset. After preprocessing, the dataset will be uploaded to our `sagemaker_session_bucket` to be used within our training job. The [imdb](http://ai.stanford.edu/~amaas/data/sentiment/) dataset consists of 25000 training and 25000 testing highly polar movie reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在该部分，我们将原来的数据处理过程，改为在sagemaker processing上面跑。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "    # load dataset\n",
    "dataset = load_dataset('imdb')\n",
    "dataset.save_to_disk('./dataset/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.set_format(\"pandas\")\n",
    "train_dataset = dataset[\"train\"][:]\n",
    "test_dataset = dataset[\"test\"][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "full_dataset = pd.concat([train_dataset, test_dataset])\n",
    "full_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.to_csv('./data/train.csv',index=False)\n",
    "test_dataset.to_csv('./data/test.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload datest_data_tfm to S3\n",
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "input_dataset_prefix = 'hf-sm-pipeline/dataset/input'\n",
    "\n",
    "dataset_s3_uri = f\"s3://{sagemaker_session_bucket}/{input_dataset_prefix}\"\n",
    "\n",
    "S3Uploader.upload('./data/', desired_s3_uri=dataset_s3_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile processing.py\n",
    "\n",
    "# Tokenization\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "input_data_path = \"/opt/ml/processing/input_data\"\n",
    "output_data_path = \"/opt/ml/processing/output_data\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--tokenizer_name\", type=str, default=\"distilbert-base-uncased\")\n",
    "    parser.add_argument(\"--dataset_name\", type=str, default=\"imdb\")\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    print(\"Received arguments {}\".format(args))\n",
    "\n",
    "    \n",
    "    # tokenizer used in preprocessing\n",
    "    tokenizer_name = args.tokenizer_name # 'distilbert-base-uncased'\n",
    "\n",
    "    # dataset used\n",
    "    dataset_name = args.dataset_name # 'imdb'\n",
    "\n",
    "    # s3 key prefix for the data\n",
    "#     s3_prefix = 'samples/datasets/imdb'\n",
    "\n",
    "    # load dataset\n",
    "#     dataset = load_dataset(dataset_name)\n",
    "\n",
    "    # download tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "    # tokenizer helper function\n",
    "    def tokenize(batch):\n",
    "        return tokenizer(batch['text'], padding='max_length', truncation=True)\n",
    "\n",
    "    # load dataset\n",
    "    train_dataset = load_dataset('csv', data_files={'train':os.path.join(input_data_path,'train.csv')})\n",
    "    test_dataset = load_dataset('csv', data_files={'test':os.path.join(input_data_path,'test.csv')})\n",
    "#     train_dataset, test_dataset = load_dataset(dataset_name, split=['train', 'test'])\n",
    "#     test_dataset = test_dataset.shuffle().select(range(10000)) # smaller the size for test dataset to 10k \n",
    "\n",
    "\n",
    "    # tokenize dataset\n",
    "    train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "    test_dataset = test_dataset.map(tokenize, batched=True)\n",
    "\n",
    "    # set format for pytorch\n",
    "    train_dataset =  train_dataset.rename_column(\"label\", \"labels\")\n",
    "    train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "    test_dataset = test_dataset.rename_column(\"label\", \"labels\")\n",
    "    test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "\n",
    "    # save dataset to /opt/ml/processing/\n",
    "    train_dataset.save_to_disk(output_data_path)\n",
    "    test_dataset.save_to_disk(output_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sagemaker.processing import (ProcessingInput, ProcessingOutput,\n",
    "                                  ScriptProcessor)\n",
    "\n",
    "processing_repository_uri = '763104351884.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch-training:1.9-transformers4.12-gpu-py38-cu111-ubuntu20.04'\n",
    "script_processor = ScriptProcessor(command=['python3'],\n",
    "                image_uri=processing_repository_uri,\n",
    "                role=role,\n",
    "                instance_count=1,\n",
    "                instance_type='ml.m5.2xlarge',\n",
    "                base_job_name=base_job_name + '-processing')\n",
    "\n",
    "prefix = 'hf-sm-pipeline/dataset'\n",
    "\n",
    "input_data = 's3://{}/{}/input'.format(sagemaker_session_bucket, prefix)\n",
    "output_data = 's3://{}/{}/output'.format(sagemaker_session_bucket, prefix)\n",
    "\n",
    "tokenizer_name = 'distilbert-base-uncased'\n",
    "dataset_name = 'imdb'\n",
    "\n",
    "script_processor.run(code='processing.py',\n",
    "                      inputs=[ProcessingInput(\n",
    "                        source=input_data,\n",
    "                        destination='/opt/ml/processing/input_data',\n",
    "                        s3_data_distribution_type='ShardedByS3Key')],\n",
    "                      outputs=[ProcessingOutput(destination=output_data,\n",
    "                                                source='/opt/ml/processing/output_data',\n",
    "                                                s3_upload_mode = 'Continuous')],\n",
    "                      arguments=['--tokenizer_name', tokenizer_name,\n",
    "                                '--dataset_name', dataset_name]\n",
    "                     )\n",
    "script_processor_job_description = script_processor.jobs[-1].describe()\n",
    "print(script_processor_job_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning & starting Sagemaker Training Job\n",
    "\n",
    "In order to create a sagemaker training job we need an `HuggingFace` Estimator. The Estimator handles end-to-end Amazon SageMaker training and deployment tasks. In a Estimator we define, which fine-tuning script should be used as `entry_point`, which `instance_type` should be used, which `hyperparameters` are passed in .....\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "huggingface_estimator = HuggingFace(entry_point='train.py',\n",
    "                            source_dir='./scripts',\n",
    "                            base_job_name='huggingface-sdk-extension',\n",
    "                            instance_type='ml.p3.2xlarge',\n",
    "                            instance_count=1,\n",
    "                            transformers_version='4.4',\n",
    "                            pytorch_version='1.6',\n",
    "                            py_version='py36',\n",
    "                            role=role,\n",
    "                            hyperparameters = {'epochs': 1,\n",
    "                                               'train_batch_size': 32,\n",
    "                                               'model_name':'distilbert-base-uncased'\n",
    "                                                })\n",
    "```\n",
    "\n",
    "When we create a SageMaker training job, SageMaker takes care of starting and managing all the required ec2 instances for us with the `huggingface` container, uploads the provided fine-tuning script `train.py` and downloads the data from our `sagemaker_session_bucket` into the container at `/opt/ml/input/data`. Then, it starts the training job by running. \n",
    "\n",
    "```python\n",
    "/opt/conda/bin/python train.py --epochs 1 --model_name distilbert-base-uncased --train_batch_size 32\n",
    "```\n",
    "\n",
    "The `hyperparameters` you define in the `HuggingFace` estimator are passed in as named arguments. \n",
    "\n",
    "Sagemaker is providing useful properties about the training environment through various environment variables, including the following:\n",
    "\n",
    "* `SM_MODEL_DIR`: A string that represents the path where the training job writes the model artifacts to. After training, artifacts in this directory are uploaded to S3 for model hosting.\n",
    "\n",
    "* `SM_NUM_GPUS`: An integer representing the number of GPUs available to the host.\n",
    "\n",
    "* `SM_CHANNEL_XXXX:` A string that represents the path to the directory that contains the input data for the specified channel. For example, if you specify two input channels in the HuggingFace estimator’s fit call, named `train` and `test`, the environment variables `SM_CHANNEL_TRAIN` and `SM_CHANNEL_TEST` are set.\n",
    "\n",
    "\n",
    "To run your training job locally you can define `instance_type='local'` or `instance_type='local_gpu'` for gpu usage. _Note: this does not working within SageMaker Studio_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an Estimator and start a training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters={'epochs': 1,\n",
    "                 'train_batch_size': 32,\n",
    "                 'model_name':'distilbert-base-uncased'\n",
    "                 }\n",
    "\n",
    "huggingface_estimator = HuggingFace(entry_point='train.py',\n",
    "                            source_dir='./scripts',\n",
    "                            instance_type='ml.p3.2xlarge',\n",
    "                            instance_count=1,\n",
    "                            role=role,\n",
    "                            transformers_version='4.12',\n",
    "                            pytorch_version='1.9',\n",
    "                            py_version='py38',\n",
    "                            hyperparameters = hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# starting the train job with our uploaded datasets as input\n",
    "\n",
    "training_input_path = output_data + '/train'\n",
    "test_input_path = output_data + '/test'\n",
    "\n",
    "huggingface_estimator.fit({'train': training_input_path, 'test': test_input_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch transform\n",
    "\n",
    "Now let's try use batch transform to inference mount of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  sagemaker.model import Model\n",
    "\n",
    "image_uri = '763104351884.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch-inference:1.9-transformers4.12-cpu-py38-ubuntu20.04'\n",
    "\n",
    "hf_model = Model(image_uri=image_uri, \n",
    "              model_data=huggingface_estimator.model_data, \n",
    "              role=role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 准备推理数据并上传到S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile test_data_tfm.jsonl\n",
    "{\"inputs\":\"I love using the new Inference DLC.\"}\n",
    "{\"inputs\":\"I love using the new Inference DLC.\"}\n",
    "{\"inputs\":\"I love using the new Inference DLC.\"}\n",
    "{\"inputs\":\"I love using the new Inference DLC.\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S3Uploader.upload('test_data_tfm.jsonl', f's3://{sagemaker_session_bucket}/hf-sm-pipeline/tfm/input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## This is another way to create batch transform job with huggingface model\n",
    "\n",
    "# from sagemaker.huggingface.model import HuggingFaceModel\n",
    "\n",
    "# huggingface_model = HuggingFaceModel(\n",
    "#     role=role, \n",
    "#     model_data=huggingface_estimator.model_data, \n",
    "#     transformers_version='4.12', \n",
    "#     pytorch_version='1.9', \n",
    "#     py_version='py38'\n",
    "# )\n",
    "\n",
    "# tfm_output = f's3://{sagemaker_session_bucket}/hf-sm-pipeline/tfm/output'\n",
    "\n",
    "# # create transformer to run a batch job\n",
    "# batch_job = huggingface_model.transformer(\n",
    "#     instance_count=1,\n",
    "#     instance_type='ml.m5.xlarge',\n",
    "#     strategy='SingleRecord',\n",
    "#     output_path=tfm_output, \n",
    "# )\n",
    "\n",
    "# test_data = f's3://{sagemaker_session_bucket}/hf-sm-pipeline/tfm/input'\n",
    "\n",
    "# # starts batch transform job and uses S3 data as input\n",
    "# batch_job.transform(\n",
    "#     data=test_data,\n",
    "#     content_type='application/json',    \n",
    "#     split_type='Line'\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 创建 SageMaker Batch Transform任务"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Hugging Face SageMaker Batch Transform](https://huggingface.co/docs/sagemaker/inference#run-batch-transform-with-transformers-and-sagemaker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tfm_output = f's3://{sagemaker_session_bucket}/hf-sm-pipeline/tfm/output'\n",
    "\n",
    "tfm = hf_model.transformer(\n",
    "    instance_count=1, \n",
    "    instance_type='ml.m5.xlarge', \n",
    "    output_path=tfm_output, \n",
    "    strategy='SingleRecord',\n",
    "#     max_concurrent_transforms=None, \n",
    "#     max_payload=None\n",
    "    )\n",
    "\n",
    "test_data = f's3://{sagemaker_session_bucket}/hf-sm-pipeline/tfm/input'\n",
    "\n",
    "tfm.transform(\n",
    "    data=test_data, \n",
    "    data_type='S3Prefix', \n",
    "    split_type='Line', #\n",
    "    content_type='application/json',#\n",
    "    wait=True, \n",
    "    logs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sagemaker.s3 import S3Downloader,S3Uploader,s3_path_join\n",
    "from ast import literal_eval\n",
    "# creating s3 uri for result file -> input file + .out\n",
    "output_file = f\"test_data_tfm.jsonl.out\"\n",
    "output_path = s3_path_join(tfm_output,output_file)\n",
    "\n",
    "# download file\n",
    "S3Downloader.download(output_path,'.')\n",
    "\n",
    "batch_transform_result = []\n",
    "with open(output_file) as f:\n",
    "    for line in f:\n",
    "        # converts jsonline array to normal array\n",
    "        line = \"[\" + line.replace(\"[\",\"\").replace(\"]\",\",\") + \"]\"\n",
    "        batch_transform_result = literal_eval(line) \n",
    "        \n",
    "# print results \n",
    "print(batch_transform_result[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy a realtime endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get model uri\n",
    "\n",
    "model_s3_uri = huggingface_estimator.model_data\n",
    "model_s3_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface.model import HuggingFaceModel\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "endpoint_name = 'realtime-hf-text'\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "   model_data=model_s3_uri,  # path to your trained SageMaker model\n",
    "   role=role,                                            # IAM role with permissions to create an endpoint\n",
    "   transformers_version=\"4.12\",                           # Transformers version used\n",
    "   pytorch_version=\"1.9\",                                # PyTorch version used\n",
    "   py_version='py38',                                    # Python version used\n",
    ")\n",
    "\n",
    "# deploy model to SageMaker Inference\n",
    "predictor = huggingface_model.deploy(\n",
    "   endpoint_name=endpoint_name,\n",
    "   initial_instance_count=1,\n",
    "   instance_type=\"ml.m5.xlarge\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example request: you always need to define \"inputs\"\n",
    "data = {\n",
    "   \"inputs\": \"Camera - You are awarded a SiPix Digital Camera! call 09061221066 fromm landline. Delivery within 28 days.\"\n",
    "}\n",
    "\n",
    "# request\n",
    "predictor.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "client = boto3.client('sagemaker-runtime')\n",
    "data = {\n",
    "   \"inputs\": \"Camera - You are awarded a SiPix Digital Camera! call 09061221066 fromm landline. Delivery within 28 days.\"\n",
    "}\n",
    "\n",
    "payload = json.dumps(data)\n",
    "response = client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=payload,\n",
    "    ContentType='application/json',\n",
    ")\n",
    "\n",
    "print(response['Body'].read().decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Delete endpoint if not need\n",
    "\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build SageMaker Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "\n",
    "sagemaker_session = sagemaker.session.Session()\n",
    "region = sagemaker_session.boto_region_name\n",
    "role = sagemaker.get_execution_role()\n",
    "sagemaker_session_bucket = sagemaker_session.default_bucket()\n",
    "model_package_group_name = f\"HuggingFacesTCModelPackageGroupName\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_prefix = 'hf-sm-pipeline/dataset'\n",
    "batch_prefix = 'hf-sm-pipeline/tfm'\n",
    "\n",
    "input_data_uri = 's3://{}/{}/input'.format(sagemaker_session_bucket, dataset_prefix)\n",
    "output_data_uri = 's3://{}/{}/output'.format(sagemaker_session_bucket, dataset_prefix)\n",
    "\n",
    "batch_data_uri = 's3://{}/{}/input'.format(sagemaker_session_bucket, batch_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterInteger,\n",
    "    ParameterString,\n",
    "    ParameterFloat,\n",
    ")\n",
    "\n",
    "\n",
    "processing_instance_count = ParameterInteger(name=\"ProcessingInstanceCount\", default_value=1)\n",
    "instance_type = ParameterString(name=\"TrainingInstanceType\", default_value=\"ml.m5.xlarge\")\n",
    "# model_approval_status = ParameterString(\n",
    "#     name=\"ModelApprovalStatus\", default_value=\"PendingManualApproval\"\n",
    "# )\n",
    "input_data = ParameterString(\n",
    "    name=\"InputData\",\n",
    "    default_value=input_data_uri,\n",
    ")\n",
    "batch_data = ParameterString(\n",
    "    name=\"BatchData\",\n",
    "    default_value=batch_data_uri,\n",
    ")\n",
    "# mse_threshold = ParameterFloat(name=\"MseThreshold\", default_value=6.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define ProcessingStep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import (ProcessingInput, ProcessingOutput,\n",
    "                                  ScriptProcessor)\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "\n",
    "processing_repository_uri = '763104351884.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch-training:1.9-transformers4.12-gpu-py38-cu111-ubuntu20.04'\n",
    "\n",
    "script_processor = ScriptProcessor(command=['python3'],\n",
    "                image_uri=processing_repository_uri,\n",
    "                role=role,\n",
    "                instance_count=1,\n",
    "                instance_type='ml.m5.2xlarge',\n",
    "                base_job_name=base_job_name + '-processing')\n",
    "\n",
    "\n",
    "tokenizer_name = 'distilbert-base-uncased'\n",
    "dataset_name = 'imdb'\n",
    "\n",
    "\n",
    "step_process = ProcessingStep(\n",
    "    name=\"TextTokenizerProcess\",\n",
    "    processor=script_processor,\n",
    "    inputs=[\n",
    "        ProcessingInput(source=input_data, destination=\"/opt/ml/processing/input_data\"),\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"train\", source=\"/opt/ml/processing/output_data/train\", destination=output_data_uri+'/train/'),\n",
    "        ProcessingOutput(output_name=\"test\", source=\"/opt/ml/processing/output_data/test\", destination=output_data_uri+'/test/'),\n",
    "    ],\n",
    "    job_arguments=['--tokenizer_name', tokenizer_name, '--dataset_name', dataset_name],\n",
    "    code=\"processing.py\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a Training Step to Train a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.workflow.steps import TrainingStep\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters={'epochs': 1,\n",
    "                 'train_batch_size': 32,\n",
    "                 'model_name':'distilbert-base-uncased'\n",
    "                 }\n",
    "\n",
    "huggingface_estimator = HuggingFace(entry_point='train.py',\n",
    "                            source_dir='./scripts',\n",
    "                            instance_type='ml.p3.2xlarge',\n",
    "                            instance_count=1,\n",
    "                            role=role,\n",
    "                            transformers_version='4.12',\n",
    "                            pytorch_version='1.9',\n",
    "                            py_version='py38',\n",
    "                            hyperparameters = hyperparameters)\n",
    "\n",
    "\n",
    "step_train = TrainingStep(\n",
    "    name=\"HuggingFaceTextClassificationTrain\",\n",
    "    estimator=huggingface_estimator,\n",
    "    inputs={\n",
    "        \"train\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\"train\"].S3Output.S3Uri,\n",
    "        ),\n",
    "        \"test\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\"test\"].S3Output.S3Uri,\n",
    "        ),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You also could add a model evaluation step here\n",
    "We will dismiss this step in demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a Create Model Step to Create a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface.model import HuggingFaceModel\n",
    "from sagemaker.inputs import CreateModelInput\n",
    "from sagemaker.workflow.steps import CreateModelStep\n",
    "\n",
    "huggingface_model = HuggingFaceModel(\n",
    "    model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts, \n",
    "    transformers_version='4.12', \n",
    "    pytorch_version='1.9', \n",
    "    py_version='py38',\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    role=role,\n",
    ")\n",
    "\n",
    "\n",
    "inputs = CreateModelInput(\n",
    "    instance_type=\"ml.p3.2xlarge\",\n",
    ")\n",
    "step_create_model = CreateModelStep(\n",
    "    name=\"HuggingFaceTextClassificationCreateModel\",\n",
    "    model=huggingface_model,\n",
    "    inputs=inputs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a Transform Step to Perform Batch Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.transformer import Transformer\n",
    "\n",
    "from sagemaker.inputs import TransformInput\n",
    "from sagemaker.workflow.steps import TransformStep\n",
    "\n",
    "batch_output = 's3://{}/{}/output'.format(sagemaker_session_bucket, dataset_prefix)\n",
    "\n",
    "transformer = Transformer(\n",
    "    model_name=step_create_model.properties.ModelName,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    instance_count=1,\n",
    "    output_path=batch_output,\n",
    "    strategy='SingleRecord',\n",
    ")\n",
    "\n",
    "\n",
    "step_transform = TransformStep(\n",
    "    name=\"HuggingFaceTextClassificationTransform\", \n",
    "    transformer=transformer, \n",
    "    inputs=TransformInput(data=batch_data,\n",
    "                         split_type='Line',\n",
    "                         content_type='application/json',)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a Pipeline of Parameters, Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "\n",
    "pipeline_name = f\"HuggingFaceTextClassificationPipeline\"\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        processing_instance_count,\n",
    "        instance_type,\n",
    "        input_data,\n",
    "        batch_data,\n",
    "    ],\n",
    "    steps=[step_process, step_train, step_create_model, step_transform],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examining the pipeline definition (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "definition = json.loads(pipeline.definition())\n",
    "definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit the pipeline to SageMaker and start execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pipeline.upsert(role_arn=role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution = pipeline.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lineage\n",
    "Review the lineage of the artifacts generated by the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sagemaker.lineage.visualizer import LineageTableVisualizer\n",
    "\n",
    "\n",
    "viz = LineageTableVisualizer(sagemaker.session.Session())\n",
    "for execution_step in reversed(execution.list_steps()):\n",
    "    print(execution_step)\n",
    "    display(viz.show(pipeline_execution_step=execution_step))\n",
    "    time.sleep(5)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "interpreter": {
   "hash": "c281c456f1b8161c8906f4af2c08ed2c40c50136979eaae69688b01f70e9f4a9"
  },
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
